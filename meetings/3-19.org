* Proposal
- Outline done
- But rabbit hole of...
  - (For the proposal mention this, but I think commit to BMA)
* BFs / marginal likelihoods vs posterior predictive distribution CV
- Same arguments as BMA vs Stacking
- Lotfi, S. et al.(2022): Bayesian model selection, the marginal likelihood, and generalization
- Yao et al. (2018): Using stacking to average bayesian predictive distributions (with discussion)
- Quentin & EJ's (2018): Limitations of bayesian leave-one-out cross-validation for model selection"
  - + discussion
- (Reread Kaplan too)
** Arguments
- Model selection methods based of marginal likelihoods aren't choosing between trained models. They're choosing between untrained models
- BMA will asymptotically select one model (the one that minimized KL divergence), while stacking will select the optimal mixture
- But stacking can lead to wonky posteriors of parameters, which is undesirable for making inferences about inclusion/exclusion of edges
  - Although I'm not sure this is relevant for multiple models with a-most-likely high number of shared edges

* For our case
- What are our goals?
  - Try to estimate the conditional independence structure of the data
    - If we formulate as inference about the existence of edges I think BMA is sensible
- But since we don't really have any beliefs about the considered models
  - Because they are model-searched in a purely exploratory way
    - We are going to make trade-offs between computational feasibility and good approximations during model search
  - Because nobody beliefs that the graphical models are (an approximation of) a data generating process (?) (?)
    - The questions we are interested on are not really about directional causal structure or effects, but questions like connectivity
- Doesn't stacking align more with our goals?
  - The performance of BMA is very dependent on the considered models
  - In our model selection procedure we are going to make significant trade-offs between computational speed & rigor
    - What is the reliability of multiple runs of Occam's window algorithm under the same conditions?
    - Testing this might be computationally not compatible with testing other stuff
      - Estimated point of reference?
  - Robustness against model search
- Although if we don't sample for our models, computing the estimates of the LOOCV meh
  - More expensive than good estimations of the marginals after model search?

