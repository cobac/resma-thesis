* Occam's Window
** Performance
- In Occam's papers they mention starting from a random model in the algorithm description and from the saturated model in their examples
  - But it's not a simulation study or anything like that, it's /just/ 3 examples
- I've tried
  - Random model :: Bad, like last week
  - Saturated model :: Bad
  - Using multiple random models ::
    - .. With linear regression it is realistic to start with the whole model space
      - 11 variables = 2047 possible models
      - (It's so funny than in the papers from the 90s they say that that would mean fitting too many models and now it takes 20s)
    - But with graphs is not going to be possible
      - So starting with /only/ 500 random models, ~1/5th of the total model space
    - So the model still over-selects parameters
      - The posterior probabilities of the true edges are 1
        - But there are some high probabilities for false edges, although not 1
        - So with an inclusion threshold of ~0.9999999 we get sensitivity and specificity of 1
    - To compensate, I can decrease the size of the window / penalize complexity more
      - It works better, reduces the probability of the false variables
      - But still... not ideal, there are false variables with posterior probabilities of +.90
        - BMA & BAS work much better
      - (It works even better when I set the size of the window to 0, but I'm not sure how the algorithm works in this case)
    - I think it's worth to finish implementing the GGM and compare with BDGraph before deciding what to do with the project
    - Also the output includes a lot of models, while BMA/BAS max ~5/10
** Our speed
- The speed results from last week were useless, since we were exploring way less models that them
- We are slower, since they use sequential calculations and don't need to fit all models
  - I've been profiling and all the runtime is basically fitting regression models
- They do ~100ms
- We do ~20seg per run with ~11k iterations
  - Each iteration 10s/100s of model comparisons
- For graphical models we are going to have to calculate the marginals no matter what unless sequential so yeahwe are going to have to calculate the marginals no matter what unless sequential so yeah
*** I think Laplace / analytical during model search are going to be unviable
- There is one /big/ optimization left that I can do, but idk if it's gonna be enough
* Alternatives 
- Leaps and bound
  - A /normal/ binary tree exploration algorithm but with very smart sequential calculations
  - Not really tractable with more than 30 variables in their use-case
  - What they do is drop the worse 30 variables at the beginning
  - Without the sequential calculations (they only need 6 operations per regression) I don't think it's not gonna be tractable at all
- Other broad model-selection algorithm?
  - Something like the sorting algorithm from rbinnet?
  - ...Maybe an unproper MCMC chain that explores the model space?
* Psychonetrics model search
