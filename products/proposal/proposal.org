#+title: Form 2A - Research Master's Psychology: Thesis Research Proposal
#+date:
#+author: 

#+BEGIN_SRC elisp :eval :results none :exports results
  (coba-define-org-tex-template)
  (setq custom-tex-template (mapconcat 'identity (list
                                                  org-tex-report-template
                                                  org-tex-math-template
                                                  org-tex-graphix-template                                                  
                                                  ) "\n"))
(coba-define-org-tex-template)
#+END_SRC

#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \setlength{\parskip}{\baselineskip}%
#+LATEX_HEADER: \setlength{\parindent}{4pt}

#+LATEX_HEADER: \defbibheading{bibliography}[7. References]{%
#+LATEX_HEADER: \section*{#1}}

#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \setlength{\headsep}{2.25\parskip}

#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \fancyhf{}
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
# #+LATEX_HEADER: \setlength\headheight{80.0pt}
# #+LATEX_HEADER: \addtolength{\textheight}{-80.0pt}
#+LATEX_HEADER: \lhead{\includegraphics[width = .40 \textwidth]{uvalogo.pdf}}
#+LATEX_HEADER: \rhead{Graduate School of Psychology}
#+LATEX_HEADER: \cfoot{\thepage}
\pagenumbering{gobble}

\thispagestyle{fancy}

\raggedright
* 1. General Information
** 1.1 Student information 
- Student name: :: David Coba
- Student Id card number: :: 12439665
- Address: :: -
- Postal code and residence: :: -
- Telephone number: :: -
- Email address: :: coba@cobac.eu
- Major: :: Psychological methods
  \newpage
** 1.2 Supervisor information
- Supervisor name: :: Maarten Marsman
- Second assesor name: :: Jonas Haslbeck
- Specialization: :: Psychological Methods
** 1.3 Other information
- Date: :: 1.04.2022
- Status: :: First draft
- Number of ECs for the thesis: :: 32EC
- Ethics Review Board (ERB) code: :: -
  
\pagenumbering{arabic}
* 2. Title and Summary of the Research Project
** 2.1 Title: Assessing the performance Occam's window for Bayesian model averaging
** 2.2 Summary of proposal 

When we select a statistical model and use it to make inferences about its parameters, we usually ignore the uncertainty derived from the model selection process leading to overconfident inferences. There are techniques that address this, like Bayesian model averaging. However, when the space of possible models is vast, such as with graphical models that are popular in psychology, it is not evident how to efficiently find the most relevant ones.
Occam's window is model search algorithm that uses heuristics to select a set of candidate models.

The goal of this project is to assess in general terms if Occam's window is a suitable method to explore the model space, specifically in the context of graphical models. To this end we will develop an Occam's window implementation and conduct a simulation study exploring how the algorithm performs under different conditions and how it compares to other alternative model search techniques.


Keywords: Bayesian inference, Bayesian model averaging, model selection, model search algorithms, Occam's window


\hfill Word count: 150/150

* 3. Project description 
# (1200 w)
# Describe prior research, a comprehensible literature review of the research field, converging upon the  research questions. 
# a) Describe the state of affairs, including the theoretical framework, in the current research field based on the existing body of literature.
# b) Clarify how the previous research eventuates into the research questions of the current proposal.

** 3.1 Prior research

# Rewrite the opening
When we perform statistical inferences, such as hypotheses tests about the inclusion of a parameter in a model or whether a parameter lays within an interval, we typically select a statistical model and then use that model to perform the inference.
However, this single-model approach underestimates the total uncertainty in our inferences, since it essentially ignores the uncertainty derived from the model selection process. And, ignoring this uncertainty, leads to overconfident conclusions \parencites{leamer1978specification}{draper1987modeluncertainty}{hoeting1999bma}[for a recent review of the issue see][]{kaplan2021quantification}.
The aim of this project in general terms is to explore whether an algorithm called Occam's window can be useful to deal the issue of single-model inference.
Specifically, we are motivated by the issue of deciding whether to include or not particular edges in graphical models that are popular in psychology. The number of possible graphical models grows exponentially with the number of variables, and current approaches to multi-model inference struggle because of the size of the model space.

Different Bayesian solutions have been proposed that allow us to model the uncertainty of the model selection process. These approaches can be categorized into two groups.
The first group is using mixture models that encompass all possible models. To estimate the joint posterior distribution of all possible models researchers usually employ simulation based methods like Markov chain Monte Carlo model composition \parencite[MC^3,][]{madigan1995mc3} or reversible jump Markov chain Monte Carlo [[parencite:&green1995rjmcmc]].
However, it is often unclear how to efficiently implement simulation based methods, and they tend to have stability issues
[[parencite:&yao2018bayesianstacking]].
The second group of approaches to multi-model inference is to combine the information only from a set of candidate models \(\mathcal A\), instead of using the whole model space. With these methods, the posterior probability of our target inference (e.g. whether a parameter is included on the model or not) given the observed data, \(p(\Delta | D)\), is a weighted average of that inference across all candidate models \(p(\Delta |M_k, D), \; M_k \in \mathcal A\). 
This approach allows to separate the use of multiple models into two steps: identifying a set of candidate models \(\mathcal A\) and then combining the uncertainty from those models.

One method to combine multiple models and not ignore the uncertainty of the model-selection process is Bayesian model averaging 
[[parencite:BMA, &leamer1978specification;&hinne2020modelaveraging;&hoeting1999bma]].
BMA uses the posterior probability of candidate models \(p(M_k | D)\) as model weights, and our target inference \(p(\Delta | D)\) becomes \[
p(\Delta | D) = \sum_{\forall M_k \in \mathcal A}^{} p(\Delta| \mathcal{M}_k, D) p(M_k | D) \text{.}
\]
From Bayes theorem we know that the posterior probability of a model is the product of the prior probability of that model \(p(M_k)\) times the marginal likelihood of the data under that model \(p(D|M_k)\), divided by the sum of that same product for all candidate models \[
p(M _k | D) = \frac{p(D | \mathcal  M_k) p(M_k)}{\sum_{\forall M_l \in \mathcal A}^{} p(D| M_l) p(M_l)} \text{.}\]
Lastly, to calculate the marginal likelihood we need to integrate the product of the likelihood function of each model \(p(D | \theta_k, M_k)\) and the prior distribution of the model parameters \(p(\theta_k | M_k)\) over the whole parameter space
\[
p(D | M_k) = \int_{}^{} p(D | \theta_k, M_k) p(\theta_k | M_k) d\theta_k \text{.}
\]

An alternative method to BMA is Bayesian model stacking parencites:&wolpert1992stacking;&yao2018bayesianstacking.
The literature is divided between proponents of marginal likelihood based methods, such as Bayes factors and BMA, and proponents of methods based on the posterior predictive distributions, such as leave-one-out cross-validation and model stacking. The disagreements between proponents of either approach seem to be rooted on differences in philosophical positions and scientific goals [[parencite:&gronau2018limloocv;parencite:&gronau2019rejoinderloocv;&lotfi2022bayesmodel;parencite:&vehtari2018limlimloocv]].

# The second method is model stacking, which minimizes the leave-one-out cross-validation (LOOCV) estimate of a loss function to assign weights to different models [[parencite:&wolpert1992stacking]].
# Stacking is a common technique to aggregate point estimations from different models, but [[textcite:&yao2018bayesianstacking]] extend the method to combine Bayesian predictive distributions, producing combined uncertainty distributions similarly to BMA. It is possible to calculate LOOCV estimates from samples of the posterior distribution [[parencite:&vehtari2016loocv]], which makes it convenient if one is using methods such as Markov chain Monte Carlo to estimate the posterior distributions in the first place.


# The main difference between BMA and model stacking is their asymptotic behavior when the data-generating model is not in the set of candidate models \(\mathcal A\).
# In this scenario, BMA will select the single model that minimizes the Kullback-Leibler divergence from the data-generating process, while model stacking will select the mixture of models that minimizes the loss function that was used to find the model weights parencite:&yao2018bayesianstacking.

# - BFs /untrained/ models vs ppd-based trained models
# - In this case our ultimate scientific goals are about the conditional dependencies structures in the data, inclusion/exclusion which edges
# - BMA more sensible to the models that are considered than stacking
# - No-one believs that a GGM or an ISING model are the data generating process
#   - maybe maarten irt idk
# - We are going to make trade-offs during the model search phase between computational feasibility and exactness
# - Stacking more robust option for model combination (?)
#   - Although posterior distribution of parameters might be wonky, we were planing on using the sum of weights (posterior model probabilities in BMA) of the models that include a particular parameter

When we do not have strong theoretical arguments to pre-select a set of candidate models \(\mathcal A\) to average with BMA, we can use a model search algorithm. One possible algorithm is the topic of this thesis: Occam's window
parencite:&madigan1994occamsgraphical;&raftery1997bmalinear,
which is based on Occam's razor principle.
Occam's razor (also known as the law of parsimony) states than when one is presented with competing hypotheses that explain equally well a particular phenomena, one should choose the simplest one.
In general terms, Occam's window algorithm first selects a set of models that fit the data reasonably well, and then discards all models that have simpler counterparts that fit the data equally well. Formally, the first step equals constructing the set of models\[
\mathcal A' = \left \{ M_k : \frac{\max \{p(M_{l} | D)\}}{p(M _k | D)} \leq c\right  \}
\]
with posterior probabilities \(p(M_k | D)\) not significantly lower 
than the model with highest posterior probability of all models \(M_l \in \mathcal A '\). The constant \(c\) specifies the range of posterior probabilities---the size of the window---that fit the data reasonably well.
For the second step the algorithm identifies the set of models \[
\mathcal B = \left\{ M_k : \exists M_l \in \mathcal A',
 M_l \subset M_k,
\frac{p(M_l | D)}{p(M_k | D)} > 1
 \right\} 
 \]
that have at least one submodel \(M_l\) in \(\mathcal A'\) with greater posterior probability.
The final set of candidate models is the set of models in the first set that are not present in the second \(\mathcal A = \mathcal A' \setminus \mathcal B\).
Computationally, the algorithm is a deterministic greedy search that performs two passes over the model space. The first pass goes from the bottom to the top (i.e. comparing the simplest models with \(p\) parameters to models with \(p+1\) parameters and so on), and the second pass starts from the most complex models and compares all the way to the simplest.
To calculate posterior model probabilities \(p(M_k|D)\) we need to compute the marginal likelihood \(p(D|M_k)\) of each model, similarly to BMA. 
However, in most cases it is not possible to calculate marginal likelihoods analytically, and we require of approximate solutions. 

# Occam's window algorithm can take advantage of sequential computations
# particularly efficient when it is possible to reuse the calculations of the marginal likelihood of a model to calculate the marginal likelihood of a model that encompasses the first. 
# [[textcite:&madigan1994occamsgraphical]] describe a procedure that allows to re-use calculations for some graphical models and [[textcite:&raftery1997bmalinear]] for linear models. The latter is implemented in the R package BMA [[parencite:&raftery2015bma]]. 

Since Occam's window uses marginal likelihoods to compare models many times during the model search, we need efficient ways of estimating or approximating them.
The first and crudest approximation is to use the Bayesian information criterion \parencites[BIC,][]{schwarz1978bic}{kass1995bayesfactors}.
The BIC of a model \(M_k\) is defined as \[
\text{BIC}(M_k) = -2 \log p\left(D | \widehat \theta, M_k \right) + d_{M_k} \log n \text{,}
\] 
where \( p\left(D | \widehat \theta, M_k\right) \) is the likelihood 
function evaluated at the maximum likelihood estimates of the model's parameters,
\(d_{Mi}\) is the number of parameters in the model and \(n\) is the sample size. textcite:&kass1995bayesfactors show that the logarithm of the marginal likelihood of a model can be approximated as \[
\log p \left( D | M_k \right) \approx
\log p\left(D | \widehat \theta, M_k\right)
-\frac{1}{2} d_{M_k} \log n \text{,}
\] 
which means that \[
\log p \left( D | M_k \right) \approx \frac{\text{BIC}(M_k)}{-2}
\] and that the ratio of marginal likelihoods between two models---the Bayes factor---is \[
2 \log B_i_j = - \text{BIC}(M_i) + \text{BIC}(M_j) \text{.}
\]
Bridge sampling offers another approach to approximate the marginal likelihood [[parencite:&gronau2017bridge;&bennett1976bridge]]. Bridge sampling generally provides accurate approximations of the marginal likelihoods, but is also very computationally demanding and not usable with a model search algorithm, because it is a simulation based method and has to draw samples.
A method between BIC and bridge sampling in terms of accuracy and computational demands is the Laplace approximation [[parencite:&lecam1953some;&kass1995bayesfactors]]. This method approximates the posterior distribution with a normal distribution centered around the posterior mode, which can be estimated using expectation-maximization algorithms. The standard Laplace approximation is accurate to the second moment of the posterior distribution, but it is possible to extend it get more accurate approximations at the cost of more computational resources or further assumptions [[parencite:&ruli2016improvedlaplace;&rue2009inla;&hubin2016inla;&tierney1989laplace;&tierney1986accurate]].
Lastly, note that in the context of Occam's window, it is possible to use a faster but less accurate approximation during model search, and use a slower but more accurate approximation during the model combination step.

One of the drawbacks of Occam's window is that it overestimates the posterior probability of the selected "best" candidate models and it underestimates ---essentially nullifies---the posterior probability of the rest of the models. This is by design and acknowledged by [[textcite:&madigan1994occamsgraphical]], and it is a trade-off we have to make to avoid having to combine information from the complete model space. Occam's window is implemented for linear regression models using priors that allow to analytically calculate the marginal likelihoods [[parencite:&raftery1997bmalinear]] in the R package BMA [[parencite:&raftery2015bma]].
There is also an extension of Occam's window to allows to model streams of data that become available sequentially [[parencite:&onorante2016dynamicow]].

# - Occam's window algorithm shines computationally if there is a way of re-using computations and update marginals sequentially

** 3.2 Key questions
# Now state the key questions, the essence of the proposal. Here, the intended research should be connected to prior research. Testable research model/ expectations/ hypotheses should be derived from the key question, and the relation between theory and research hypotheses should be clearly specified.
# a) Formulate a general relevant research question based on previous research.
# b) Translate the general research question in a clear manner into a specific research question.
# c) Translate the specific research questions into testable research model/ expectations/ hypotheses.

The goals of this project are to develop an Occam's window implementation for graphical models that are popular in psychological research, like the Gaussian graphical model (GGM) and the Ising model, and benchmark its performance.
To this end we will first implement Occam's window algorithm for simpler models, such as linear regression and logistic regression. 
Later, we will explore with a simulation study the possible trade-offs between accuracy and computational speed of different marginal likelihood approximations, and also how Occam's window performance compares to alternative model search algorithms.

 \hfill Word count: 1291/1200
  
* 4. Procedure 
# (1000 w)

** 4.1 Operationalization
# Describe how the research questions are operationalized. 
# a) Operationalize the research questions in a clear manner into a research design/strategy. 
# b) Describe the procedures for conducting the research and collecting the data. 
# c) For methodological and/or simulation projects describe the design of the simulation study. 

To address our research questions we will first implement Occam's window model search algorithm in steps, and then conduct a simulation study. We plan on implementing our algorithm and running our simulations in the Julia programming language [[parencite:&Julia]].
There are more simulation conditions that are potentially interesting than how many we can realistically tackle during this project, and the number of conditions that we can test will depend on how smoothly the project progresses.

Regarding which models to use during our simulations, linear regression is the obvious simplest choice to start developing the algorithm. Logistic regression is a next step that increases the complexity of the procedure, and the GGM and the Ising model are the ones that motivate this project. First, we will implement Occam's window algorithm using the BIC approximation for the marginal likelihood, since it is the simplest method and it will allow us to test our implementation while developing it. Next, for linear regression models and the GGM there are convenient prior distributions for the model parameters that allow to calculate the marginal likelihoods analytically. Finally, for the logistic and Ising models we will have to implement Laplace approximations of the marginal likelihoods.

Alternative model search algorithms to Occam's window include Bayesian adaptive sampling (BAS) and birth-death Markov chain Monte Carlo (BDMCMC). BAS samples without replacement from the space of possible models and uses the marginal likelihoods of the sampled models to iteratively estimate the marginal likelihoods of the models that remain unsampled [[parencite:&clyde2011bas]]. BAS is available for (generalized) linear  models as an R package [[parencite:&clyde2021bas]]. BDMCMC [[parencite:&mohammadi2015bdgraph]] samples from the joint posterior space of all possible models, and uses a Poisson process to model the rate at which the Markov chains jump from one model to another. BDMCMC is available in the R package BDGraph [[parencite:&mohamamadi2019bdgraph]] for graphical models, which uses a pseudo-likelihood funciton [[parencite:&pensar2017marginalpseudo]] and an analytical approximation to the ratio of marginal likelihoods [[parencite:&mohammadi2017accelarating]].
We will only implement Occam's window algorithm, and rely on the implementations of BAS for linear models and BDgraph for graphical models as benchmarks.

# Taking this into consideration, these are broadly speaking the conditions we will prioritize testing:
# 
# 1. Occam's window with linear regression models and BIC approximation.
# 2. Occam's window with linear regression models and Laplace approximation.
# 3. Occam's window with logistic regression models and Laplace approximation.
# 4. Occam's window with Gaussian graphical models and BIC approximation.
# 5. Occam's window with Gaussian graphical models and Laplace approximation.
# 6. BAS with its current implementation in R.
# 7. BDgraph with its current implementation in R.
# 8. Occam's window with Ising models and BIC approximation.
# 9. Occam's window with Ising models and Laplace approximation.
# 10. Using Occam's window model search with BIC, re-run BMA but using the Laplace approximation.
# 11. Using Occam's window model search with BIC, re-run BMA but using bridge sampling.
# 
# We believe that it is realistic to complete up to condition no. 9 in this project. Evaluating the performance of conditions no. 10 and no. 11 will most likely remain open questions for future research.

** 4.2 Sample characteristics
# d) In case of a simulation study, indicate how data will be generated

We plan on generating data from a set of models and evaluating how well each simulation condition recovers the characteristics of the true data-generating models.
In general terms, we will consider conditions with different sample sizes and sparsity levels in the covariance matrices of the data-generating models.
However, we do not think it makes sense to commit to specific data-generating processes at this stage of the project.
   
** 4.4 Data analysis
 # Describe the data preprocessing. Indicate for each research question separately, how it is translated into a statistical prediction. For example: “In a repeated measures ANOVA we expect an interaction effect of the between factor x and the within factor y on the dependent variable z. Also indicate how you will correct for multiple comparisons. Only the analyses proposed here can be described as confirmatory analyses in your research report. All other have to be mentioned as exploratory. 

This project is inherently exploratory and, similarly to the last section, we do not think it makes sense to commit at this stage to a specific analysis plan. In general terms, to assess how well each model search algorithm performs we will use the posterior probabilities of including specific edges that are (or not) present in the data-generating model, in terms of sensitivity and specificity.
To assess computational costs we will use real runtime in order to not penalize algorithms that benefit from parallel computations. If instead we used CPU time, we would be penalizing all parallelizable algorithms by a factor of the number of parallel processes or threads.

** 4.4 Modifiability of procedure
# Is there room for modification of the intended procedure? Evaluation of the proposal by the RMP Thesis Committee is meaningful only if the recommendations that the Committee might have can be implemented. It is therefore required that the intended procedure can be modified before you start gathering data. In situations where procedures or operationalization’s or sample characteristics cannot be modified, the Thesis Committee has to be consulted before handing in the research proposal. The committee will consider the eligibility of this project for a research thesis. 

The scope of this project is highly flexible, and we can adapt which conditions to include or exclude in our simulation study depending on how fast we progress. In section 6.1 "Time schedule" we detail the milestones we aim to complete before certain deadlines. 
 
\hfill Word count: 568/1000

* 5. Intended results 
# (250 w)
# Clarify what the implication of possible outcomes would be (per hypothesis) for the specific and general research questions as well as for the theory. Address the following in approximately 250 
# words:
# a) What are the interpretations if the results do  match the expectations? 
# b) What are the interpretations if the results do not match the expectations?
# c) Are there any alternative interpretations?
# d) Is there any practical or societal relevance? Please explain. 

The main goal of this project is to assess in general terms how Occam's window performs. 
If our analysis concludes that the algorithm compares favorably against alternative methods, we will show that Occam's window can be a useful tool to supplement the use of BMA to avoid the problem of single-model inference.
We are motivated specially by the case of graphical models, where the space of possible models grows exponentially with the number of variables. 
Current approaches to sampling from the complete model space have limitations, and we anticipate that Occam's window can be a useful tool that is currently underused.
In case that our results show that the performance of Occam's window does not compensate for its shortcomings, we would have provided an updated assessment of its performance that is currently lacking in the literature.
To our knowledge there are no simulation studies evaluating how Occam's window performs under different conditions, or how it compares to other model search algorithms.
Moreover, we expect to contribute software that implements BMA and Occam's window, and that integrates with the rest of the Julia ecosystem. 

\hfill Word count: 184/250

* 6. Work plan
# (500w)
# Describe how the research project will be executed. Who is doing what and when? Is the planning of the current project realistic, efficient and feasible?
** 6.1 Time schedule
# State the total amount of EC as noted in the thesis contract (26-32EC excl. proposal), 1EC stands for 28 hours work. Present and justify a time schedule in weeks, including your time investment in hours per week. Plan some spare time, and indicate what elements can be cut / reduced if necessary. Provide the intended presentation date.

This thesis project consists of 28 EC, excluding the thesis proposal. This is equivalent to approximately 18 weeks working full time. We aim to complete and present the project by the 15th of July 2022. In broad terms we plan to achieve the following milestones each month:

- April :: 
  - Week 1-3: Address feedback on the proposal and implement Occam's window algorithm for linear regression models using BIC as an approximation to the marginal likelihood.
  - Week 4: Implement analytical evaluations of the marginal likelihood for linear regression models.
- May :: 
  - Week 1: Buffer time and hopefully enjoy the UvA teaching-free days.
  - Week 3: Implement analytical evaluations of the marginal likelihood for Gaussian graphical models.
  - Week 3: Buffer time and start running simulations, including with BAS and BDGraph.
  - Week 4: Continue running simulations and implement the Laplace approximation for logistic models.
- June :: 
  - Week 1: Continue running simulations and implement the Laplace approximation for Ising models.
  - Week 2: Continue running simulations and start analyzing results. Start writing the thesis.
  - Week 3/4: Analyze results and thesis writing. Complete a first draft of the full thesis.
- July :: 
  - Weeks 1/2: Complete writing the thesis and prepare the presentation.

The scope of this project is highly flexible, and we can adapt which conditions to include or exclude in our simulation study depending on how fast we progress.

** 6.2 Infrastructure
# Where will the research take place? How is access to the facilities and materials ensured?

No special infrastructure is required to complete this project.
** 6.3 Data storage
# Each researcher needs to comply with the storage protocol of the Research Institute Psychology: http://psyres.uva.nl/content/scientific-integrity-docs/data-protocol.html 

We will keep the results of all our simulations under version control and with remote backups. We do not plan on collecting any data, and in the case we end up deciding to use empirical data we would use publicly available datasets.

** 6.3 Budget
# The compensation from the department is max € 55 for each research project. If the total expenditure exceeds the maximum compensation, then specify how the surplus will be financed. The budget may be used for travel expenses, participant payment. Specify the financial ramifications for the intended research. Another € 25 budget may be used for printing costs (e.g. for the conference poster). Please go to the secretariat of the specialization of your supervisor with your receipts. 

In principle we will not require extra funds to complete this project. In the case that the computational resources that we have access to prove insufficient to conduct the simulations, we might consider using cloud computing services. In any case, such costs would not exceed the maximum budget.

\hfill Word count: 323/500

\printbibliography

* 8. Further steps
Make sure your supervisor submits an Ethics Checklist for your intended research to the Ethics Review Board of the Department of Psychology at https://www.lab.uva.nl/lab/ethics/
* 7. Signatures
- [X] I hereby declare that both this proposal, and its resulting thesis, will only contain original material and is free of plagiarism (cf. Teaching and Examination Regulation in the research master’s course catalogue).
- [X] I hereby declare that the result section of the thesis will consist of two subsections, one entitled “confirmatory analyses” and one entitled “exploratory analyses” (one of the two subsections may be empty):
  1. The confirmatory analysis section reports exactly the analyses proposed in Section 4 of this proposal.
  2. The exploratory analysis section contains not previously specified, and thus exploratory, proposal analyses. 
  
\centering
*Location:* \hspace{1cm} *Student’s signature:* \hspace{1cm} *Supervisor’s signature:*

\raggedright
\hspace{1.5cm} Amsterdam
